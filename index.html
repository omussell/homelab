<h1 id="homelab">Homelab</h1>
<h3 id="bootstrapping-a-secure-infrastructure">Bootstrapping a Secure Infrastructure</h3>
<ul>
<li><a href="/homelab/design/overview.html">Overview</a></li>
<li><a href="/homelab/design/design.html">Design</a></li>
<li><a href="/homelab/design/implementation.html">Implementation</a></li>
</ul>
<h2 id="hardware">Hardware</h2>
<pre><code>Virtualisation Host
Gigabyte Brix Pro GB-BXI7-4770R
- Intel Core i7-4770R (quad core 3.2GHz)
- 16GB RAM
- 250GB mSATA SSD
- 250GB 2.5 inch SSD

NAS
HP ProLiant G8 Microserver G1610T
- Intel Celeron G1610T (dual core 2.3 GHz)
- 16GB RAM
- 2 x 250GB SSD
- 2 x 3TB HDD

Management
Raspberry Pi 2 Model B
- Quad core 1GB RAM
- 8GB MicroSD (w/ NOOBS)</code></pre>
<h2 id="automating-freebsd-jail-creation">Automating FreeBSD Jail Creation</h2>
<pre><code>Creating the template:
zfs create -o mountpoint=/usr/local/jails zroot/jails
zfs create -p zroot/jails/template

Download the base files into a new directory
mkdir ~/jails
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/10.2-RELEASE/base.txz -o ~/jails
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/10.2-RELEASE/lib32.txz -o ~/jails
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/10.2-RELEASE/ports.txz -o ~/jails

Extract the base files
tar -xf ~/jails/base.txz -C /usr/local/jails/template
tar -xf ~/jails/lib32.txz -C /usr/local/jails/template
tar -xf ~/jails/ports.txz -C /usr/local/jails/template

Copy files from host to template
cp /etc/resolv.conf /usr/local/jails/template/etc/resolv.conf
cp /etc/localtime /usr/local/jails/template/etc/localtime
mkdir -p /usr/local/template/home/username/.ssh
cp /home/username/.ssh/authorized_keys /usr/local/jails/template/home/username/.ssh

When finished, take a snapshot
zfs snapshot zroot/jails/template@1

Create the new jails zfs dataset
zfs clone zroot/jails/template@1 zroot/jails/testjail

Edit /etc/jail.conf
# Global settings applied to all jails

interface = &quot;re0&quot;;
host.hostname = &quot;$name&quot;;
ip4.addr = 192.168.1.$ip;
path = &quot;/usr/local/jails/$name&quot;;

exec.start = &quot;/bin/sh /etc/rc&quot;;
exec.stop = &quot;/bin/sh /etc/rc.shutdown&quot;;
exec.clean;
mount.devfs;

# Jail Definitions
testjail {
    $ip = 15;
}

Run the jail
jail -c testjail

View running jails
jls

Login to the jail
jexec $jailname sh</code></pre>
<h2 id="new-template-script">New Template Script</h2>
<pre><code>#! /bin/sh

# Create mountpoint
zfs create -o mountpoint=/usr/local/jails zroot/jails
zfs create -p zroot/jails/template

# Copy pre-downloaded base files
scp -r &quot;user@freenas:/mnt/SSD_storage/jails/*.txz&quot; /tmp

# Extract the files to the template location
tar -xf /tmp/base.txz -C /usr/local/jails/template
tar -xf /tmp/lib32.txz -C /usr/local/jails/template
tar -xf /tmp/ports.txz -C /usr/local/jails/template

# Copy files from host/hypervisor to template
cp /etc/resolv.conf /usr/local/jails/template/etc/resolv.conf
cp /etc/localtime /usr/local/jails/template/etc/localtime
mkdir -p /usr/local/template/home/user/.ssh
cp /home/user/.ssh/authorized_keys /usr/local/jails/template/home/user/.ssh

# Create the snapshot
zfs snapshot zroot/jails/template@1

# Copy the pre-configured jail.conf file
scp -r &quot;user@freenas:/mnt/SSD_storage/jails/jail.conf&quot; /usr/local/jails/template/etc/

echo &quot;Run the jails using jail -c jailname&quot;
echo &quot;View running jails with jls&quot;
echo &quot;Log into the jail using jexec jailname sh&quot;</code></pre>
<h2 id="new-jail-creation-script">New Jail Creation Script</h2>
<pre><code>#! /bin/sh
read -p &quot;Enter jail name:&quot; hostname
zfs clone zroot/jails/template@1 zroot/jails/$hostname
echo hostname=\&quot;$hostname\&quot; &gt; /usr/local/jails/$hostname/etc/rc.conf
# Disable sendmail to speed up start time
echo sendmail_submit_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$hostname/etc/rc.conf
echo sendmail_outbound_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$hostname/etc/rc.conf
echo sendmail_msp_queue_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$hostname/etc/rc.conf</code></pre>
<h2 id="automating-bhyve-vm-creation">Automating Bhyve VM Creation</h2>
<pre><code>Edit /etc/sysctl.conf
net.link.tap.up_on_open=1

Edit /boot/loader.conf
vmm_load=&quot;YES&quot;
nmdm_load=&quot;YES&quot;
if_bridge_load=&quot;YES&quot;
if_tap_load=&quot;YES&quot;

Edit /etc/rc.conf
cloned_interfaces=&quot;bridge0 tap0&quot;
ifconfig_bridge0=&quot;addm re0 addm tap0&quot;

Create ZFS volume
zfs create -V16G -o volmode=dev zroot/testvm

Download the installation image
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/ISO-IMAGES/10.2/FreeBSD-10.2-RELEASE-amd64-disc1.iso 

Start the VM
sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/testvm -i -I FreeBSD-10.2-RELEASE-amd64-disc1.iso testvm

Install as normal, following the menu options</code></pre>
<h2 id="new-vm-creation-script">New VM Creation Script</h2>
<pre><code>#! /bin/sh
read -p &quot;Enter hostname: &quot; hostname
zfs create -V16G -o volmode=dev zroot/$hostname
sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/$hostname -i -I ~/FreeBSD-10.2-RELEASE-amd64-disc1.iso $hostname</code></pre>
<h2 id="creating-a-linux-guest">Creating a Linux guest</h2>
<pre><code>Create a file for the hard disk
truncate -s 16G linux.img

Create the file to map the virtual devices for kernel load
cat device.map (use the full path)
(hd0) /root/linux.img
(cd0) /root/linux.iso

Load the kernel
grub-bhyve -m device.map -r cd0 -M 1024M linuxguest

Grub should start, choose install as normal

Start the VM
bhyve -A -H -P -s 0:0,hostbridge -s 1:0,lpc -s 2:0,virtio-net,tap0 -s 3:0,virtio-blk,/root/linux.img -l com1,/dev/nmdm0A -c 1 -m 512M linuxguest

Access through the serial console
cu -l /dev/nmdm0B</code></pre>
<h2 id="backing-up-vms">Backing up VMs</h2>
<pre><code>In the FreeNAS web interface, create a new user
Account
Add User
Fill in the fields
Allow sudo
Ensure the public key is pasted into the SSH key field

Connect to the NAS over SSH
# sysctl vfs.usermount=1
# echo vfs.usermount=1 &gt;&gt; /etc/sysctl.conf
# zfs create SSD_storage/backup
# zfs allow -u user create,mount,receive SSD_storage/backup

Allow user to send snapshots
# zfs allow -u user send,snapshot zroot

Create the snapshot
$ zfs snapshot zroot/testvm@1

Send the snapshot to the NAS
$ zfs send zroot/testvm@1 | ssh user@freenas zfs recv -dvu SSD_storage/backup

After a full snapshot is taken, incremental backups can be performed
$ zfs send -i zroot/testvm@1 zroot/testvm@2 | ssh user@freenas zfs recv -dvu SSD_storage/backup</code></pre>
<h2 id="pfsense-in-a-vm">pfSense in a VM</h2>
<pre><code>Download the pfSense disk image from the website using fetch
fetch https://frafiles.pfsense.org/mirror/downloads/pfSense-CE-2.3.1-RELEASE-2g-amd64-nanobsd.img.gz -o ~/pfSense.img.gz

Create the storage
zfs create -V2G -o volmode=dev zroot/pfsense

Unzip the file, and redirect output to the storage via dd
gzip -dc pfSense.img.gz | dd of=/dev/zvol/zroot/pfsense obs=64k

Load the kernel and start the boot process
bhyveload -c /dev/nmdm0A -d /dev/zvol/zroot/pfsense -m 256MB pfsense

Start the VM
/usr/sbin/bhyve -c 1 -m 256 -A -H -P -s 0:0,hostbridge -s 1:0,virtio-net,tap0 -s 3:0,ahci-hd,/dev/zvol/zroot/pfsense -s 4:1,lpc -l com1,/dev/nmdm0A pfsense

Connect to the VM via the serial connection with nmdm
cu -l /dev/nmdm0B

Perform initial configuration through the shell to assign the network interfaces

Once done, use the IP address to access through the web console 

When finished, you can shutdown/reboot

To de-allocate the resources, you need to destroy the VM
bhyvectl --destroy --vm=pfsense</code></pre>
<h2 id="nanobsd">NanoBSD</h2>
<pre><code>NanoBSD docs
PDF: NanoBSD, ZFS and Jails

cd to NanoBSD build directory
cd /usr/src/tools/tools/nanobsd

Run the build script, using default values for now
sh nanobsd.sh

Wait a while for buildworld to finish

Go to output directory
cd /usr/obj/nanobsd.full

Create a new ZFS device
zfs create -V5G -o volmode=dev zroot/testnano

Copy the NanoBSD full image to the ZFS device
dd if=_.disk.full of=/dev/zvol/zroot/testnano

Run Bhyve using the ZFS device for boot
sh /usr/share/examples/bhyve/vmrun.sh -c 1 -m 512M -t tap0 -d /dev/zvol/zroot/testnano testnano

It boots! But, it cant mount root on a device... yet....

***
Got it to boot by changing the NANO_DRIVE variable from ad0 to vtbd0 in the
nanobsd.sh file

Cloning NanoBSD vms
zfs snapshot zroot/nanotest@1
zfs clone zroot/nanotest@1 zroot/nanoclone
***

nanobsdv.conf

cust_nobeastie() (
    touch ${nano_worlddir}/boot/loader.conf
    echo &quot;beastie_disable=\&quot;yes\&quot;&quot; &gt;&gt; ${nano_worlddir}/boot/loader.conf
)

customize_cmd cust_install_files
customize_cmd cust_nobeastie

Got internet and zfs working
Check that the files are configured exactly as in the handbook
Make sure to run ifconfig tap0 up and ifconfig bridge0 up
For zfs, the NANO_MODULES variable needs zfs and opensolaris added e.g.
NANO_MODULES=&quot;zfs opensolaris&quot;
Simply adding this to the conf file was causing errors, needed to
build kernel and world again.
Also, it kept saying the usual &quot;filesystem is full error&quot;, but this time
the file system really was full.
Added &quot;FlashDevice SanDisk 4G&quot; in the conf file to give it 4GB instead 
of 1GB.
Compiled it again and it works now.

sh nanobsdv.sh -b -c nanobsdv.conf

Base system:
Hostname + IP
Change root password using OPIE
Create admin user with random password
Create SSH keys
Remove root login access
Setup security: 
TCP wrapper
Firewall
IDS
Pull config from gold server
Start jails/applications</code></pre>
<h2 id="multiple-vms-using-bhyve">Multiple VMs using bhyve</h2>
<p>To allow networking on multiple vms, there should be a tap assigned to each vm, connected to the same bridge.</p>
<p>So to set up the bridge and an initial tap interface:</p>
<pre><code>Edit /etc/sysctl.conf
net.link.tap.up_on_open=1

Edit /boot/loader.conf
vmm_load=&quot;YES&quot;
nmdm_load=&quot;YES&quot;
if_bridge_load=&quot;YES&quot;
if_tap_load=&quot;YES&quot;

Edit /etc/rc.conf
cloned_interfaces=&quot;bridge0 tap0&quot;
ifconfig_bridge0=&quot;addm re0 addm tap0&quot;

You then add multiple tap interfaces, by adding them to cloned_interfaces and ifconfig_bridge0 in /etc/rc.conf.
cloned_interfaces=&quot;bridge0 tap0 tap1 tap2&quot;
ifconfig_bridge0=&quot;addm re0 addm tap0 addm tap1 addm tap2&quot;

Then when you provision vms, assign one of the tap interfaces to them.</code></pre>
<h2 id="jails-setup-in-nanobsd-bhyve-vm">Jails setup in NanoBSD bhyve vm</h2>
<pre><code>jail.conf | listofjails | newtemplate.sh | createalljails | startalljails.sh</code></pre>
<h2 id="jails-infrastructure">Jails Infrastructure</h2>
<pre><code>createalljails.sh

cat /etc/jail.conf | grep -E \{$ | sed &quot;s/{//&quot; &gt; ~/jails/listofjails.txt
#}} - ignore these braces, the previous command messes with the syntax higlighting

for jail in `cat ~/jails/listofjails.txt`
do
    zfs clone zroot/jails/template@1 zroot/jails/$jail
    echo hostname=$jail &gt; /usr/local/jails/$jail/etc/rc.conf

    # Disable sendmail to speed up start time
    echo sendmail_submit_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$jail/etc/rc.conf
    echo sendmail_outbound_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$jail/etc/rc.conf
    echo sendmail_msp_queue_enable=\&quot;NO\&quot; &gt;&gt; /usr/local/jails/$jail/etc/rc.conf

cat ~/jails/listofjails.txt | sed &quot;s/^/jail \-c /&quot;</code></pre>
<h2 id="etcjail.conf">/etc/jail.conf</h2>
<pre><code># Global settings applied to all jails

interface = &quot;re0&quot;;
host.hostname = &quot;$name&quot;;
ip4.addr = 192.168.1.$ip;
path = &quot;/usr/local/jails/$name&quot;;

exec.start = &quot;/bin/sh /etc/rc&quot;;
exec.stop = &quot;/bin/sh /etc/rc.shutdown&quot;;
exec.clean;
mount.devfs;

# Jail Definitions

wintermute {
    $ip = 15;
}

neuromancer {
    $ip = 16;
}

armitage {
    $ip = 17;
}

finn {
    $ip = 18;
}

hideo {
    $ip = 19;
}

maelcum {
    $ip = 20;
}

case {
    $ip = 21;
}

molly {
    $ip = 22;
}</code></pre>
<h2 id="jail-names">Jail names</h2>
<pre><code>Armitage
- Version control - Git
- Host install tools - Ansible, shell scripts
- Ad hoc change tools - Ansible, shell scripts

Wintermute - Neuromancer
- Directory servers -- DNS, NIS, LDAP
- Authentication servers -- NIS, Kerberos 
- Time Synchronisation -- NTP
- Host IP addressing -- DHCP

Finn
- Network File servers -- NFS
- File Replication servers -- Git, Ansible, SUP

Maelcum
- Mail -- SMTP

Hideo
- Logging -- Syslogd
- Security -- OPIE, SSH, PKI
- Performance monitoring -- collectd

Case - Molly
- Web servers -- Apache</code></pre>
<h2 id="updating-jails">Updating Jails</h2>
<pre><code>Update the host system first:
freebsd-update fetch
freebsd-update install

Find out current version:
$VERSION=freebsd-version | sed &#39;s/-/ /&#39; | awk &#39;{print $1}&#39;

Fetch the base files from the FreeBSD FTP:
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/$VERSION-RELEASE/base.txz -o ~/jails
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/$VERSION-RELEASE/lib32.txz -o ~/jails
fetch ftp://ftp.freebsd.org/pub/FreeBSD/releases/amd64/amd64/$VERSION-RELEASE/ports.txz -o ~/jails

Remove the previous template and all cloned jails
zfs destroy -R zroot/jails/template@1

Create the template directory:
zfs create -p zroot/jails/template

Extract the downloaded base files:
tar -xf ~/jails/*.txz -C /usr/local/jails/template

Copy needed files:
cp /etc/resolv.conf /usr/local/jails/template/etc
mkdir -p /usr/local/jails/template/home/username/.ssh
cp /home/username/.ssh/authorized_keys /usr/local/jails/template/home/username/.ssh

Create the new snapshot:
zfs snapshot zroot/jails/template@1

Run the createalljails.sh script

Run the startalljails.sh script</code></pre>
<h2 id="thin-jails">Thin jails</h2>
<pre><code>Update the template
freebsd-update -b /usr/local/jails/template fetch install</code></pre>
<h2 id="ansible-setup">Ansible Setup</h2>
<pre><code># Create the &quot;ansible&quot; user
adduser -f filename
ansible:::::::::password
# Generate the SSH key for the ansible user
ssh-keygen -N &quot;&quot; -f /home/ansible/.ssh/id_rsa
su -m ansible -c &#39;ssh-keygen -N &quot;&quot; -f /home/ansible/.ssh/id_rsa&#39;
cat /home/ansible/.ssh/id_rsa.pub

echo &quot;ansible:::::::::$RANDOMPASSWORD&quot; &gt; /usr/local/jails/$JAILNAME/root/ansibleuser
jexec $JAILENAME adduser -f /root/ansibleuser

rm /usr/local/jails/$JAILNAME/root/ansibleuser</code></pre>
<h2 id="crypto">Crypto</h2>
<pre><code>Supersingular isogeny Diffie-Hellman key exchange (SIDH)
Efficient algorithms for SIDH (PDF)
SIDH library

Instant Messaging client
Irssi IRC client with OTR for authentication
Irssi-OTR
Irssi
OTR
DANE</code></pre>
<h2 id="dns">DNS</h2>
<pre><code>Unbound working as an authoritative resolver for LAN. You need to run 
unbound-control reload after changing the conf file, otherwise it wont work!
You also need to change /etc/resolv.conf so that nameserver 127.0.0.1 
appears ABOVE the bthomehub resolver, so that the local unbound server 
is queried first before it goes to bthomehub. (in prod resolv.conf would just
have the proper name servers, but this can stay for now)
unbound.conf

You also need to make sure that local-* is inside the server: block, 
otherwise it doesnt work...
server:
    unblock-lan-zones: yes
    username stuff...
    interface: 0.0.0.0
    local-zone: &quot;local.&quot; static
    local-data: &quot;finn.local. A 192.168.1.18&quot;
    
NSD (name server daemon) is an authoritative only, memory efficient, highly
secure and simple to configure open source domain name server. NSD acts as 
the authoritative name server, while Unbound acts as the validating, 
resolving and caching DNS server.

The Unbound servers act as validating, recursive and caching DNS servers
that LAN clients can query. Then NSD is an authoritative server which
can resolve internal LAN names only. NSD never goes to the internet,
and its only job is to serve internal names to Unbound.

Zones can be signed with the OpenDNSSEC tool. Private keys associated
with DNSSEC signing are secured using HSMs (hardware security modules).
This can be done using OpenHSM for testing, however, in production a 
real HSM would be used.</code></pre>
<h2 id="ssh">SSH</h2>
<pre><code># Jails ssh config
# Remove existing host keys and generate ed25519 host key
if [ ! -f /etc/ssh/ssh_host_ed25519_key ]; then
    rm /etc/ssh/ssh_*host*
    ssh-keygen -f /etc/ssh/ssh_host_ed25519_key -t ed25519 -N &quot;&quot; -E sha256
fi

# Generate SSHFP records
ssh-keygen -r $(hostname) | awk &#39;$4 == &amp;&amp; $5 == 2 {print $0}&#39;</code></pre>
<h2 id="mount-zfs-datasets-over-nfs">Mount ZFS datasets over NFS</h2>
<pre><code>zfs create zroot/test
zfs set sharenfs=&quot;rw=@192.168.1/24&quot; zroot/test</code></pre>
<h2 id="ssh-with-x.509-v3-certificate-support-pkix-ssh">SSH with X.509 v3 Certificate Support (PKIX-SSH)</h2>
<pre><code>Running inside a jail for testing

fetch http://roumenpetrov.info/secsh/src/pkixssh-9.2.tar.xz
tar -xvf pkixssh-9.2.tar.xz
vi Makefile.in
Uncomment the SHELL variable line
./configure
make
make install
Follow the instructions in README.privsep
mkdir /var/empty/
chown root:sys /var/empty/
chmod 755 /var/empty/
pw groupadd sshd
pw usermod sshd -c &#39;sshd privsep&#39; -d /var/empty -s /bin/false sshd
The default install location was /usr/local/bin
Test generating a key:
/usr/local/bin/ssh-keygen -b 384 -t ecdsa -f /etc/ssh/ssh_host_key -N &quot;&quot;</code></pre>
<h2 id="compiling-nginx-with-chacha20-support">Compiling NGINX with ChaCha20 support</h2>
<pre><code># Make a working directory
mkdir ~/nginx
cd ~/nginx

# Install some dependencies
pkg install ca_root_nss
pkg install pcre
pkg install perl5

# Pull the source files
fetch https://nginx.org/download/nginx-1.13.0.tar.gz
fetch https://www.openssl.org/source/openssl-1.1.0e.tar.gz

# Extract the tarballs
tar -xzvf nginx-1.13.0.tar.gz
tar -xzvf openssl-1.1.0e.tar.gz
rm *.tar.gz

# Compile openssl
cd ~/nginx/openssl-1.1.0e.tar.gz
./config
make
make install

# openssl should default to /usr/local/bin unless prefixdir variable has been specified
/usr/local/bin/openssl version
# Should output OpenSSL 1.1.0e

# Compile NGINX
# Use the compile script listing modules to include
#!/bin/sh
cd ~/nginx/nginx-1.13.0/
#make clean

./configure \
    --with-http_ssl_module \
#   --with-http-spdy_module \
    --with-http_gzip_static_module \
    --with-file-aio \
    --with-ld-opt=&quot;-L /usr/local/lib&quot; \

#   --without-http_autoindex_module \
    --without-http_browser_module \
    --without-http_fastcgi_module \
    --without-http_geo_module \
    --without-http_map_module \
    --without-http_proxy_module \
    --without-http_memcached_module \
    --without-http_ssi_module \
    --without-http_userid_module \
    --without-http_split_clients_module \
    --without-http_uwsgi_module \
    --without-http_scgi_module \
    --without-http_limit_conn_module \
    --without-http_referer_module \
    --without-http_http-cache \
    --without_upstream_ip_hash_module \
    --without-mail_pop3_module \
    --without-mail-imap_module \
    --without-mail_smtp_module

    --with-openssl=~/nginx/openssl-1.1.0e/

make
make install

# After running the compile script, NGINX should be installed in /usr/local/nginx
# Start the service
/usr/local/nginx/sbin/nginx

# If there are no issues, update the config file in /usr/local/nginx/conf/nginx.conf. 
# Reload NGINX to apply the new config
/usr/local/nginx/sbin/nginx -s reload

# Generate an EC certificate
/usr/local/bin/openssl ecparam -list_curves
/usr/local/bin/openssl ecparam -name secp384r1 -genkey -param_enc explicit -out private-key.pem
/usr/local/bin/openssl req -new -x509 -key private-key.pem -out server.pem -days 365
cat private-key.pem server.pem &gt; server-private.pem



# Currently having trouble getting a ECDSA signed certificate to work when 
loading the site in a browser. So far it works with TLSv1.2, ECDHE_RSA, 
X25519 and CHACHA20-POLY1305. At the moment if I generate a ECDSA cert 
and use it, the site fails to load at all. So using RSA for now.

# Current NGINX config:

worker_processes  1;

events {
    worker_connections  1024;
}


http {
    include       mime.types;
    default_type  application/octet-stream;
    sendfile        on;
    keepalive_timeout  65;

    server {
        listen       80;
        server_name  localhost;
        location / {
            root   /usr/local/www/;
            index  index.html index.htm;
        }

        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }

    }

    server {
        listen       443 ssl;
        server_name  localhost;

    ssl on;
        #ssl_certificate      /root/nginx/server.pem;
        #ssl_certificate_key  /root/nginx/private.pem;
    ssl_certificate /usr/local/www/nginx-selfsigned.crt;
    ssl_certificate_key /usr/local/www/nginx-selfsigned.key;
    #ssl_ciphers HIGH;
    ssl_ciphers &quot;ECDHE-RSA-CHACHA20-POLY1305&quot;;
        ssl_prefer_server_ciphers  on;
    ssl_protocols TLSv1.2;
    ssl_ecdh_curve X25519;
    
    location / {
            root   /usr/local/www/;
            index  index.html index.htm;
        }
    }

}</code></pre>
<h2 id="nsdunbound-setup">NSD+Unbound setup</h2>
<p>Set up the unbound/nsd-control</p>
<p><code>local-unbound-setup</code></p>
<p><code>nsd-control-setup</code></p>
<p>sysrc nsd_enable=&quot;YES&quot; sysrc local_unbound_enable=&quot;YES&quot;</p>
<p>nsd.conf</p>
<p>server: port: 5353</p>
<p>/usr/local/etc/nsd/home.lan.zone</p>
<pre><code>$ORIGIN home.lan. ;
$TTL 86400 ;

@ IN SOA ns1.home.lan. admin.home.lan. (
        2017080619 ;
        28800 ;
        7200 ;
        864000 ;
        86400 ;
        )

        NS ns1.home.lan.

ns1 IN A 192.168.1.15
jail IN A 192.168.1.15</code></pre>
<p>/usr/local/etc/nsd/home.lan.reverse</p>
<pre><code>$ORIGIN home.lan.
$TTL 86400

0.1.168.192.in-addr.arpa. IN SOA ns1.home.lan. admin.home.lan. (
        2017080619
        28800
        7200
        864000
        86400
        )

        NS ns1.home.lan.

15.1.168.192.in-addr.arpa. IN PTR jail
15.1.168.192.in-addr.arpa. IN PTR ns1</code></pre>
<p>pkg install -y opendnssec pkg install -y softhsm</p>
<p>Edit /usr/local/etc/softhsm.conf 0:/var/lib/softhsm/slot0.db</p>
<p>Initialise the token database: softhsm --init-token --slot 0 --label &quot;OpenDNSSEC&quot; Enter the PIN for the SO and then the USER.</p>
<p>Make sure opendnssec has permission to access the token database: chown opendnssec /var/lib/softhsm/slot0.db chgrp opendnssec /var/lib/softhsm/slot0.db</p>
<p>Edit /usr/local/etc/opendnssec/conf.xml</p>
<pre><code>&lt;Repository name=&quot;SoftHSM&quot;&gt;
        &lt;Module&gt;/usr/local/lib/softhsm/libsofthsm.so&lt;/Module&gt;
        &lt;TokenLabel&gt;OpenDNSSEC&lt;/TokenLabel&gt;
        &lt;PIN&gt;1234&lt;/PIN&gt;
        &lt;SkipPublicKey/&gt;
&lt;/Repository&gt;</code></pre>
<p>Edit /usr/local/etc/opendnssec/kasp.xml. Change unixtime to datecounter in the Serial parameter.</p>
<p>This allows us to use YYYYMMDDXX format for the SOA SERIAL values.</p>
<pre><code>&lt;Zone&gt;
        &lt;PropagationDelay&gt;PT300S&lt;/PropagationDelay&gt;
        &lt;SOA&gt;
                &lt;TTL&gt;PT300S&lt;/TTL&gt;
                &lt;Minimum&gt;PT300S&lt;/Minimum&gt;
                &lt;Serial&gt;datecounter&lt;/Serial&gt;
        &lt;/SOA&gt;
&lt;/Zone&gt;</code></pre>
<h2 id="saltstack-install-and-config">SaltStack Install and Config</h2>
<pre><code>pkg install -y py27-salt
cp -v /usr/local/etc/salt/master{.sample,&quot;&quot;}
cp -v /usr/local/etc/salt/minion{.sample,&quot;&quot;}
sysrc salt_master_enable=&quot;YES&quot;
sysrc salt_minion_enable=&quot;YES&quot;

Salt expects state files to exist in the /srv/salt or /etc/salt directories which don&#39;t exist by default on FreeBSD so make symlinks instead:
mkdir -p /srv /usr/local/etc/salt/states
ln -s /usr/local/etc/salt /etc/salt
ln -s /usr/local/etc/salt /srv/salt

service salt_master onestart
service salt_minion onestart
salt-key -A
Press y

Create a test file:
vi /usr/local/etc/salt/states/examples.sls

*In yaml format*
install_packages:
  pkg.installed:
    - pkgs:
      - vim-lite

Then to run:
salt &#39;\*&#39; state.apply examples</code></pre>
<h2 id="salt-formulas">Salt Formulas</h2>
<p>Install the GitFS backend, this is allows you to serve files from git repos.</p>
<pre><code>pkg install -y py27-pygit2
Edit the /usr/local/etc/salt/master configuration file:
fileserver_backend:
  - git
  - roots
gitfs_remotes:
  - https://github.com/saltstack-formulas/lynis-formula
service salt_master onerestart
*If master and minion are the same node, restart the minion service as well*
Then in the state file
include:
  - lynis
*In this case, the lynis formula defaults to /usr/local/lynis, you may want to change this in production*
To run:
/usr/local/lynis/lynis audit system -Q
Results are ouput to /var/log/lynis-report.dat</code></pre>
<h2 id="salt-equivalent-to-r10k-and-using-git-as-a-pillar-source">Salt equivalent to R10K and using git as a pillar source</h2>
<p>If the git server is also a minion, you can use Reactor to signal to the master to update the fileserver on each git push: <code>https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#refreshing-gitfs-upon-push</code></p>
<p>You can also use git as a pillar source (host your specific config data in version control) <code>https://docs.saltstack.com/en/latest/topics/tutorials/gitfs.html#using-git-as-an-external-pillar-source</code></p>
<h2 id="installing-raet">Installing RAET</h2>
<p>The instructions below are incorrect, as the salt_master service isnt starting. This needs some investigative work, as it seems to be a freebsd thing...</p>
<pre><code>pkg install -y libsodium py27-libnacl py27-ioflo py27-raet
Edit /srv/salt/master and /srv/salt/minion and add:
transport: raet
Then restart the services:
service salt_master onerestart
service salt_minion onerestart
salt-key 
salt-key -A

If you get the error &quot;No buffer space available&quot; follow the instructions at https://github.com/saltstack/salt/issues/23196 to change the kern.ipc.maxsockbuf value. The services will also need restarting, then continue with the key acceptance.</code></pre>
<p>UPDATE: RAET support isn't enabled in the default package. If you install py27-salt and run <code>pkg info py27-salt</code> you can see in the options <code>RAET: off</code>. In order to use RAET, you need to build the py27-salt port.</p>
<pre><code>pkg remove -y py27-salt
portsnap fetch extract
cd /usr/ports/sysutil/py-salt
make config
# Press space to select RAET
make install
Edit /srv/salt/master and /srv/salt/minion and add:
transport: raet
Then restart the services:
service salt_master restart
service salt_minion restart
salt-key 
salt-key -A</code></pre>
<h2 id="salt-equivalent-of-hiera-eyaml">Salt equivalent of hiera-eyaml</h2>
<p>Salt.runners.nacl</p>
<p>Similar to hiera-eyaml, it is used for encrypting data stored in pillar: <code>https://docs.saltstack.com/en/latest/ref/runners/all/salt.runners.nacl.html</code></p>
<h2 id="install-gogs-self-hosted-git">Install Gogs (self-hosted git)</h2>
<pre><code>pkg install -y go git gcc
pw useradd git -m
su - git
GOPATH=$HOME/go; export GOPATH
echo &#39;GOPATH=$HOME/go; export GOPATH&#39; &gt;&gt; ~/.profile
cc=gcc go get -u --tags sqlite github.com/gogits/gogs
ln -s go/src/github.com/gogits/gogs gogs
cd gogs
CC=gcc go build --tags sqlite
mkdir -p custom/conf
vim custom/conf/app.ini</code></pre>
<p>custom/conf/app.ini</p>
<pre><code>RUN_USER = git
RUN_MODE = prod

[database]
DB_TYPE = sqlite3
PATH = data/gogs.db

[repository]
ROOT = /home/git/gogs-repositories
SCRIPT_TYPE = sh

[server]
DOMAIN = localhost
ROOT_URL = http://localhost/
HTTP_PORT = 3000
LANDING_PAGE = explore

[session]
PROVIDER = file

[log]
MODE = file

[security]
INSTALL_LOCK = true
SECRET_KEY = supersecret
</code></pre>
<p>To run, as the git user run /home/git/go/src/github.com/gogs/gogs web</p>
<p>In the github.com folder, there is a scripts directory which contains init scripts for various OSes to start gogs as a normal service. In addition, there is a supervisor program file. So to run gogs with supervisor:</p>
<pre><code>pkg install -y py27-supervisor
cat /home/git/go/src/github.com/gogits/gogs/scripts/supervisor/gogs &gt;&gt; /usr/local/etc/supervisord.conf
sysrc supervisord_enable=&quot;YES&quot;
supervisord -c /usr/local/etc/supervisord.conf
supervisorctl -c /usr/local/etc/supervisord.conf</code></pre>
<p>That being said, I didn't get supervisor to work... and used the init script instead.</p>
<pre><code>cp -v /home/git/go/src/github.com/gogits/gogs/scripts/init/freebsd/gogs /etc/rc.d
I needed to amend the gogs_directory path to be /home/git/go/src/github.com/gogits/gogs
chmod 555 /etc/rc.d/gogs
sysrc gogs_enable=&quot;YES&quot;
service gogs start
</code></pre>
<h2 id="install-jenkins-ci">Install Jenkins (CI)</h2>
<pre><code>pkg install -y jenkins
sysrc jenkins_enable=&quot;YES&quot;
service jenkins start</code></pre>
<p>Access via a browser at http://$IP:8180/jenkins</p>
<h2 id="install-buildbot">Install buildbot</h2>
<pre><code>pkg install -y buildbot buildbot-www
buildbot create-master master
cd ./master
cp master.cfg.sample master.cfg
buildbot start master
(look at twistd.log if there are errors during startup)

Access via a browser at http://$IP:8010/</code></pre>
<pre><code>sysrc buildbot_enable=&quot;YES&quot;
sysrc buildbot_basedir=&quot;/var/www/buildbot&quot;
service buildbot start</code></pre>
<p>If using the localworker for testing: <code>pkg install -y buildbot-worker</code></p>
<p>With postgres backend:</p>
<pre><code>master.cfg
c[&#39;db&#39;] = {
    &#39;db_url&#39; : &quot;postgresql://buildbotuser:testpass@localhost/buildbotdb&quot;,
}

pkg install -y postgresql96-server
# Follow the instructions it gives about running initdb
sysrc postgresql_enable=YES
service postgresql start

pip install psycopg2

# Create the database
createdb buildbotdb -h localhost -U postgres

# Amend pg_hba.conf in /var/db/postgres

psql -U postgres
\connect buildbotdb
create role buildbotuser</code></pre>
<h2 id="install-hugo-and-basic-site">Install Hugo and basic site</h2>
<pre><code># note &#39;hugo&#39; is not the right package. It is completely different and 
# will take a long time to download before you realise its the wrong thing.
pkg install -y gohugo git</code></pre>
<pre><code>git clone your files to the server
Run `hugo` in the directory to build the assets, which will be placed into the public directory. 
Run `hugo server --baseUrl=/ --port=1313 --appendPort=false`
Note that the baseURL is /. This is because it wasn&#39;t rendering the css at all when I used a server name or IP address. In production, this should be the domain name of the website followed by a forward slash.
You can then visit your server at port 1313. 
For the baseUrl when using github pages, you should use the repo name surrounded by slashes, like /grim/.</code></pre>
